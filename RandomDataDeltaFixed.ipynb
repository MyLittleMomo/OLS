{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM3ksvF8944rGi7RX1my5RC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vlpvB7E6FtDi","executionInfo":{"status":"ok","timestamp":1669284703657,"user_tz":-360,"elapsed":329,"user":{"displayName":"Altair Azamat","userId":"06784603574440203264"}},"outputId":"b790ed97-9eab-4bd6-d6a0-c9a145a9b0f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Coefs: \n"," [ 7.81906926  8.45763496  1.99344104 -0.14618078  8.39270814 10.42123417\n","  7.4309686   0.75858877  9.74721209  6.04018557]\n","Mean squared error: 1485.97\n","Real error RH =  1485.9657969274685\n","Loss function bound M = 14392.910408223388\n","Empirical risk RS =  1222.124077911228\n","Error bound =  9283.096702732\n","Difference between Error bound and Real error =  7797.130905804532\n"]}],"source":["#I have used the same numer of datapoints and features as in the Real Data in order to compare the results better\n","from sklearn import random\n","from sklearn import datasets\n","import math\n","import numpy as np\n","from sklearn import linear_model\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn import linear_model\n","delta=0.01 #fixed small delta to have error bound formula working with almost 100% probability\n","m=400 #parameter that you can vary between [10-400], [20,400] is preferred (usually number of datapoints is larger that number features)\n","d=10 #parameter that you can vary between [1-10]\n","x,y, coef=datasets.make_regression(n_samples=442, n_features=10, n_informative=10, noise=10, coef=True, random_state=0)#random data with 442 datapoints and 10 features which will be the same each time because of (random_state=0)\n","x=np.interp(x,(x.min(),x.max()), (0,50)) #scaled the feature data so that min=0 and max 50\n","y=np.interp(y, (y.min(), y.max()),(1000,2000)) #scaled so that min=1000 max=2000\n","x_train= x[:m,-d:]\n","y_train= y[:m]\n","x_test= x[-42:,-d:] #last 42 datapoints are used as test sample\n","y_test= y[-42:]\n","model = linear_model.LinearRegression(fit_intercept=False)\n","model.fit(x_train, y_train)\n","print('Coefs: \\n', model.coef_)\n","y_pred =model.predict(x_test)\n","print(\"Mean squared error: %.2f\" % mean_squared_error(y_test,y_pred))\n","RH = mean_squared_error(y_test, y_pred)\n","print(\"Real error RH = \",RH)\n","w_star=model.coef_\n","#print(w_star)\n","L=[None]*(len(x_train));\n","for i in range(0,len(x_train)):\n","  L[i]=(abs(np.dot(w_star, x_train[i])-y_train[i]))*(abs(np.dot(w_star, x_train[i])-y_train[i]))\n","  #print(\"Error on training point \",i+1,\" = \",L[i])\n","M=max(L);\n","print('Loss function bound M =',M)\n","LH=0;\n","for j in range(0,len(x_train)):\n","  LH=LH+L[j];\n","RS=abs(LH/len(x_train));\n","print('Empirical risk RS = ',RS);\n","ErrorBound=RS+M*math.sqrt((2*d*math.log(math.e*m/d))/m) +M*math.sqrt((math.log(1/delta))/(2*m))\n","print('Error bound = ',ErrorBound);\n","TEST=ErrorBound-RH;\n","print(\"Difference between Error bound and Real error = \",TEST);"]}]}